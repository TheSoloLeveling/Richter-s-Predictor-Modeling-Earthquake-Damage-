{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rapport.docx', 'submission_format.csv', 'test_values.csv', 'train_labels.csv', 'train_values.csv']\n"
     ]
    }
   ],
   "source": [
    "DIR  = \"data/\"\n",
    "SEED = 1881\n",
    "\n",
    "if not os.path.isdir(\"models/\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "print(os.listdir(DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(DIR+\"train_values.csv\")\n",
    "train_y = pd.read_csv(DIR+\"train_labels.csv\")\n",
    "test_x  = pd.read_csv(DIR+\"test_values.csv\")\n",
    "sub_csv = pd.read_csv(DIR+\"submission_format.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic Location ID Embedding w/ Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo1 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_1_id\"], test_x[\"geo_level_1_id\"]])))\n",
    "geo2 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_2_id\"], test_x[\"geo_level_2_id\"]])))\n",
    "geo3 = np.array(pd.get_dummies(pd.concat([train_x[\"geo_level_3_id\"], test_x[\"geo_level_3_id\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347469, 11861)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NET():\n",
    "    inp = Input( shape=(geo3.shape[1],))\n",
    "    i1 = Dense(16, name=\"intermediate\")(inp)\n",
    "    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)\n",
    "    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)\n",
    "    \n",
    "    model = Model(inp, [x2,x1])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2715/2715 - 15s - loss: 0.2057 - dense_loss: 0.0532 - dense_1_loss: 0.1525\n",
      "Epoch 2/10\n",
      "2715/2715 - 16s - loss: 0.0537 - dense_loss: 0.0052 - dense_1_loss: 0.0486\n",
      "Epoch 3/10\n",
      "2715/2715 - 16s - loss: 0.0142 - dense_loss: 0.0041 - dense_1_loss: 0.0101\n",
      "Epoch 4/10\n",
      "2715/2715 - 15s - loss: 0.0066 - dense_loss: 0.0035 - dense_1_loss: 0.0031\n",
      "Epoch 5/10\n",
      "2715/2715 - 15s - loss: 0.0043 - dense_loss: 0.0030 - dense_1_loss: 0.0012\n",
      "Epoch 6/10\n",
      "2715/2715 - 16s - loss: 0.0030 - dense_loss: 0.0024 - dense_1_loss: 5.7560e-04\n",
      "Epoch 7/10\n",
      "2715/2715 - 15s - loss: 0.0020 - dense_loss: 0.0017 - dense_1_loss: 2.9792e-04\n",
      "Epoch 8/10\n",
      "2715/2715 - 16s - loss: 0.0013 - dense_loss: 0.0011 - dense_1_loss: 1.5836e-04\n",
      "Epoch 9/10\n",
      "2715/2715 - 15s - loss: 8.4721e-04 - dense_loss: 7.6138e-04 - dense_1_loss: 8.5835e-05\n",
      "Epoch 10/10\n",
      "2715/2715 - 17s - loss: 5.7838e-04 - dense_loss: 5.3077e-04 - dense_1_loss: 4.7616e-05\n"
     ]
    }
   ],
   "source": [
    "model = NET()\n",
    "model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=2)\n",
    "model.save(\"geo_embed.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GEO-Embed Model\n",
    "model = NET()\n",
    "model.load_weights(\"geo_embed.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Extract Intermediate Layer\" Function\n",
    "from keras import backend as K\n",
    "\n",
    "get_int_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo3Expanded = tf.expand_dims(geo3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GEO-Embeds for all train data points.\n",
    "# Then assign with train_data\n",
    "\n",
    "\n",
    "out = []\n",
    "for dat in geo3Expanded[:260601]:\n",
    "    layer_output = get_int_layer_output([[dat]])[0]\n",
    "    out.append(layer_output)\n",
    "\n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "train_data = pd.get_dummies(train_x.copy())\n",
    "train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "train_data = train_data.assign(geo_feat1=out[:,0],\n",
    "                               geo_feat2=out[:,1],\n",
    "                               geo_feat3=out[:,2],  \n",
    "                               geo_feat4=out[:,3],\n",
    "                               geo_feat5=out[:,4],    \n",
    "                               geo_feat6=out[:,5],\n",
    "                               geo_feat7=out[:,6],\n",
    "                               geo_feat8=out[:,7],\n",
    "                               geo_feat9=out[:,8],\n",
    "                               geo_feat10=out[:,9],\n",
    "                               geo_feat11=out[:,10],\n",
    "                               geo_feat12=out[:,11],\n",
    "                               geo_feat13=out[:,12],\n",
    "                               geo_feat14=out[:,13],\n",
    "                               geo_feat15=out[:,14],           \n",
    "                               geo_feat16=out[:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <th>has_superstructure_mud_mortar_brick</th>\n",
       "      <th>...</th>\n",
       "      <th>geo_feat7</th>\n",
       "      <th>geo_feat8</th>\n",
       "      <th>geo_feat9</th>\n",
       "      <th>geo_feat10</th>\n",
       "      <th>geo_feat11</th>\n",
       "      <th>geo_feat12</th>\n",
       "      <th>geo_feat13</th>\n",
       "      <th>geo_feat14</th>\n",
       "      <th>geo_feat15</th>\n",
       "      <th>geo_feat16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>802906</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844075</td>\n",
       "      <td>1.629543</td>\n",
       "      <td>-1.541913</td>\n",
       "      <td>2.062092</td>\n",
       "      <td>-0.730574</td>\n",
       "      <td>0.891505</td>\n",
       "      <td>-0.115479</td>\n",
       "      <td>1.107712</td>\n",
       "      <td>-0.831422</td>\n",
       "      <td>-1.401730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28830</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.631102</td>\n",
       "      <td>-0.406130</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>1.796202</td>\n",
       "      <td>-1.391089</td>\n",
       "      <td>1.531671</td>\n",
       "      <td>-0.643732</td>\n",
       "      <td>-0.822086</td>\n",
       "      <td>-1.545030</td>\n",
       "      <td>0.019457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94947</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.986496</td>\n",
       "      <td>1.353110</td>\n",
       "      <td>-2.093531</td>\n",
       "      <td>1.864320</td>\n",
       "      <td>0.532991</td>\n",
       "      <td>1.786665</td>\n",
       "      <td>-1.018599</td>\n",
       "      <td>1.354901</td>\n",
       "      <td>0.208749</td>\n",
       "      <td>-1.243128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590882</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129595</td>\n",
       "      <td>0.732131</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>1.121952</td>\n",
       "      <td>-1.730679</td>\n",
       "      <td>1.098826</td>\n",
       "      <td>-1.571630</td>\n",
       "      <td>0.912978</td>\n",
       "      <td>-0.981634</td>\n",
       "      <td>0.832327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201944</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.846991</td>\n",
       "      <td>1.724917</td>\n",
       "      <td>-1.710472</td>\n",
       "      <td>1.022410</td>\n",
       "      <td>0.794807</td>\n",
       "      <td>1.875630</td>\n",
       "      <td>-0.017773</td>\n",
       "      <td>0.062014</td>\n",
       "      <td>-1.933042</td>\n",
       "      <td>-1.983573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "0       802906                    2   30                6                  5   \n",
       "1        28830                    2   10                8                  7   \n",
       "2        94947                    2   10                5                  5   \n",
       "3       590882                    2   10                6                  5   \n",
       "4       201944                    3   30                8                  9   \n",
       "\n",
       "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \\\n",
       "0                             1                                    1   \n",
       "1                             0                                    1   \n",
       "2                             0                                    1   \n",
       "3                             0                                    1   \n",
       "4                             1                                    0   \n",
       "\n",
       "   has_superstructure_stone_flag  has_superstructure_cement_mortar_stone  \\\n",
       "0                              0                                       0   \n",
       "1                              0                                       0   \n",
       "2                              0                                       0   \n",
       "3                              0                                       0   \n",
       "4                              0                                       0   \n",
       "\n",
       "   has_superstructure_mud_mortar_brick  ...  geo_feat7  geo_feat8  geo_feat9  \\\n",
       "0                                    0  ...   0.844075   1.629543  -1.541913   \n",
       "1                                    0  ...  -1.631102  -0.406130   0.601423   \n",
       "2                                    0  ...  -1.986496   1.353110  -2.093531   \n",
       "3                                    0  ...  -0.129595   0.732131  -0.381742   \n",
       "4                                    0  ...  -1.846991   1.724917  -1.710472   \n",
       "\n",
       "   geo_feat10  geo_feat11  geo_feat12  geo_feat13  geo_feat14  geo_feat15  \\\n",
       "0    2.062092   -0.730574    0.891505   -0.115479    1.107712   -0.831422   \n",
       "1    1.796202   -1.391089    1.531671   -0.643732   -0.822086   -1.545030   \n",
       "2    1.864320    0.532991    1.786665   -1.018599    1.354901    0.208749   \n",
       "3    1.121952   -1.730679    1.098826   -1.571630    0.912978   -0.981634   \n",
       "4    1.022410    0.794807    1.875630   -0.017773    0.062014   -1.933042   \n",
       "\n",
       "   geo_feat16  \n",
       "0   -1.401730  \n",
       "1    0.019457  \n",
       "2   -1.243128  \n",
       "3    0.832327  \n",
       "4   -1.983573  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
       "       'height_percentage', 'has_superstructure_adobe_mud',\n",
       "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'count_families', 'has_secondary_use', 'has_secondary_use_agriculture',\n",
       "       'has_secondary_use_hotel', 'has_secondary_use_rental',\n",
       "       'has_secondary_use_institution', 'has_secondary_use_school',\n",
       "       'has_secondary_use_industry', 'has_secondary_use_health_post',\n",
       "       'has_secondary_use_gov_office', 'has_secondary_use_use_police',\n",
       "       'has_secondary_use_other', 'land_surface_condition_n',\n",
       "       'land_surface_condition_o', 'land_surface_condition_t',\n",
       "       'foundation_type_h', 'foundation_type_i', 'foundation_type_r',\n",
       "       'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_q',\n",
       "       'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_m',\n",
       "       'ground_floor_type_v', 'ground_floor_type_x', 'ground_floor_type_z',\n",
       "       'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s',\n",
       "       'other_floor_type_x', 'position_j', 'position_o', 'position_s',\n",
       "       'position_t', 'plan_configuration_a', 'plan_configuration_c',\n",
       "       'plan_configuration_d', 'plan_configuration_f', 'plan_configuration_m',\n",
       "       'plan_configuration_n', 'plan_configuration_o', 'plan_configuration_q',\n",
       "       'plan_configuration_s', 'plan_configuration_u',\n",
       "       'legal_ownership_status_a', 'legal_ownership_status_r',\n",
       "       'legal_ownership_status_v', 'legal_ownership_status_w', 'geo_feat1',\n",
       "       'geo_feat2', 'geo_feat3', 'geo_feat4', 'geo_feat5', 'geo_feat6',\n",
       "       'geo_feat7', 'geo_feat8', 'geo_feat9', 'geo_feat10', 'geo_feat11',\n",
       "       'geo_feat12', 'geo_feat13', 'geo_feat14', 'geo_feat15', 'geo_feat16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GEO-Embeds for all test data points.\n",
    "# Then assign with test_data\n",
    "\n",
    "out = []\n",
    "for dat in geo3Expanded[260601:]:\n",
    "    layer_output = get_int_layer_output([[dat]])[0]\n",
    "    out.append(layer_output)\n",
    "\n",
    "out = np.array(out)\n",
    "out = np.squeeze(out)\n",
    "\n",
    "test_data = pd.get_dummies(test_x.copy())\n",
    "test_data = test_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)\n",
    "test_data = test_data.assign(geo_feat1=out[:,0],\n",
    "                             geo_feat2=out[:,1],\n",
    "                             geo_feat3=out[:,2],  \n",
    "                             geo_feat4=out[:,3],\n",
    "                             geo_feat5=out[:,4],    \n",
    "                             geo_feat6=out[:,5],\n",
    "                             geo_feat7=out[:,6],\n",
    "                             geo_feat8=out[:,7],\n",
    "                             geo_feat9=out[:,8],\n",
    "                             geo_feat10=out[:,9],\n",
    "                             geo_feat11=out[:,10],\n",
    "                             geo_feat12=out[:,11],\n",
    "                             geo_feat13=out[:,12],\n",
    "                             geo_feat14=out[:,13],\n",
    "                             geo_feat15=out[:,14],           \n",
    "                             geo_feat16=out[:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>has_superstructure_adobe_mud</th>\n",
       "      <th>has_superstructure_mud_mortar_stone</th>\n",
       "      <th>has_superstructure_stone_flag</th>\n",
       "      <th>has_superstructure_cement_mortar_stone</th>\n",
       "      <th>has_superstructure_mud_mortar_brick</th>\n",
       "      <th>...</th>\n",
       "      <th>geo_feat7</th>\n",
       "      <th>geo_feat8</th>\n",
       "      <th>geo_feat9</th>\n",
       "      <th>geo_feat10</th>\n",
       "      <th>geo_feat11</th>\n",
       "      <th>geo_feat12</th>\n",
       "      <th>geo_feat13</th>\n",
       "      <th>geo_feat14</th>\n",
       "      <th>geo_feat15</th>\n",
       "      <th>geo_feat16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300051</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.675000</td>\n",
       "      <td>0.614349</td>\n",
       "      <td>-1.389414</td>\n",
       "      <td>1.320463</td>\n",
       "      <td>-1.732539</td>\n",
       "      <td>1.060726</td>\n",
       "      <td>0.554130</td>\n",
       "      <td>0.308889</td>\n",
       "      <td>-0.575239</td>\n",
       "      <td>0.375221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99355</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259388</td>\n",
       "      <td>1.013949</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.511640</td>\n",
       "      <td>0.242684</td>\n",
       "      <td>0.535861</td>\n",
       "      <td>-0.683099</td>\n",
       "      <td>0.727221</td>\n",
       "      <td>-1.412074</td>\n",
       "      <td>-1.150415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890251</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427380</td>\n",
       "      <td>-0.299714</td>\n",
       "      <td>0.249709</td>\n",
       "      <td>0.200205</td>\n",
       "      <td>-1.337208</td>\n",
       "      <td>0.851066</td>\n",
       "      <td>-0.681611</td>\n",
       "      <td>0.188826</td>\n",
       "      <td>-1.073590</td>\n",
       "      <td>-0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.792493</td>\n",
       "      <td>2.004001</td>\n",
       "      <td>-2.410599</td>\n",
       "      <td>-0.576685</td>\n",
       "      <td>-0.977422</td>\n",
       "      <td>2.359995</td>\n",
       "      <td>-1.858793</td>\n",
       "      <td>0.477453</td>\n",
       "      <td>1.662198</td>\n",
       "      <td>-0.735731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>421793</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598990</td>\n",
       "      <td>1.364992</td>\n",
       "      <td>-1.006493</td>\n",
       "      <td>1.054921</td>\n",
       "      <td>-1.392896</td>\n",
       "      <td>1.093669</td>\n",
       "      <td>0.717468</td>\n",
       "      <td>1.190503</td>\n",
       "      <td>-1.531369</td>\n",
       "      <td>0.445548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "0       300051                    3   20                7                  6   \n",
       "1        99355                    2   25               13                  5   \n",
       "2       890251                    2    5                4                  5   \n",
       "3       745817                    1    0               19                  3   \n",
       "4       421793                    3   15                8                  7   \n",
       "\n",
       "   has_superstructure_adobe_mud  has_superstructure_mud_mortar_stone  \\\n",
       "0                             0                                    1   \n",
       "1                             0                                    1   \n",
       "2                             0                                    1   \n",
       "3                             0                                    0   \n",
       "4                             0                                    1   \n",
       "\n",
       "   has_superstructure_stone_flag  has_superstructure_cement_mortar_stone  \\\n",
       "0                              0                                       0   \n",
       "1                              0                                       0   \n",
       "2                              0                                       0   \n",
       "3                              0                                       0   \n",
       "4                              0                                       0   \n",
       "\n",
       "   has_superstructure_mud_mortar_brick  ...  geo_feat7  geo_feat8  geo_feat9  \\\n",
       "0                                    0  ...  -0.675000   0.614349  -1.389414   \n",
       "1                                    0  ...   0.259388   1.013949   0.518523   \n",
       "2                                    0  ...  -0.427380  -0.299714   0.249709   \n",
       "3                                    0  ...  -1.792493   2.004001  -2.410599   \n",
       "4                                    0  ...   0.598990   1.364992  -1.006493   \n",
       "\n",
       "   geo_feat10  geo_feat11  geo_feat12  geo_feat13  geo_feat14  geo_feat15  \\\n",
       "0    1.320463   -1.732539    1.060726    0.554130    0.308889   -0.575239   \n",
       "1    0.511640    0.242684    0.535861   -0.683099    0.727221   -1.412074   \n",
       "2    0.200205   -1.337208    0.851066   -0.681611    0.188826   -1.073590   \n",
       "3   -0.576685   -0.977422    2.359995   -1.858793    0.477453    1.662198   \n",
       "4    1.054921   -1.392896    1.093669    0.717468    1.190503   -1.531369   \n",
       "\n",
       "   geo_feat16  \n",
       "0    0.375221  \n",
       "1   -1.150415  \n",
       "2   -0.246500  \n",
       "3   -0.735731  \n",
       "4    0.445548  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
       "       'height_percentage', 'has_superstructure_adobe_mud',\n",
       "       'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
       "       'has_superstructure_cement_mortar_stone',\n",
       "       'has_superstructure_mud_mortar_brick',\n",
       "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
       "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
       "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
       "       'count_families', 'has_secondary_use', 'has_secondary_use_agriculture',\n",
       "       'has_secondary_use_hotel', 'has_secondary_use_rental',\n",
       "       'has_secondary_use_institution', 'has_secondary_use_school',\n",
       "       'has_secondary_use_industry', 'has_secondary_use_health_post',\n",
       "       'has_secondary_use_gov_office', 'has_secondary_use_use_police',\n",
       "       'has_secondary_use_other', 'land_surface_condition_n',\n",
       "       'land_surface_condition_o', 'land_surface_condition_t',\n",
       "       'foundation_type_h', 'foundation_type_i', 'foundation_type_r',\n",
       "       'foundation_type_u', 'foundation_type_w', 'roof_type_n', 'roof_type_q',\n",
       "       'roof_type_x', 'ground_floor_type_f', 'ground_floor_type_m',\n",
       "       'ground_floor_type_v', 'ground_floor_type_x', 'ground_floor_type_z',\n",
       "       'other_floor_type_j', 'other_floor_type_q', 'other_floor_type_s',\n",
       "       'other_floor_type_x', 'position_j', 'position_o', 'position_s',\n",
       "       'position_t', 'plan_configuration_a', 'plan_configuration_c',\n",
       "       'plan_configuration_d', 'plan_configuration_f', 'plan_configuration_m',\n",
       "       'plan_configuration_n', 'plan_configuration_o', 'plan_configuration_q',\n",
       "       'plan_configuration_s', 'plan_configuration_u',\n",
       "       'legal_ownership_status_a', 'legal_ownership_status_r',\n",
       "       'legal_ownership_status_v', 'legal_ownership_status_w', 'geo_feat1',\n",
       "       'geo_feat2', 'geo_feat3', 'geo_feat4', 'geo_feat5', 'geo_feat6',\n",
       "       'geo_feat7', 'geo_feat8', 'geo_feat9', 'geo_feat10', 'geo_feat11',\n",
       "       'geo_feat12', 'geo_feat13', 'geo_feat14', 'geo_feat15', 'geo_feat16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_arr(array):\n",
    "    # Get major confidence-scored predicted value.\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val))))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tuf F15\\Desktop\\projet\\main.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tuf%20F15/Desktop/projet/main.ipynb#ch0000031?line=0'>1</a>\u001b[0m \u001b[39m# Add Target variable to training data \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tuf%20F15/Desktop/projet/main.ipynb#ch0000031?line=1'>2</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mdamage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mtrain_y[\u001b[39m'\u001b[39m\u001b[39mdamage_grade\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Dataset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# Add Target variable to training data \n",
    "y = np.array(train_y[\"damage_grade\"])-1\n",
    "df = train_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "x['damage']=y['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of target variable to find out if there is class imbaance problem\n",
    "x['damage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for volumne percentage \n",
    "x['volume_percentage']=x['area_percentage']* x['height_percentage']\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y variables\n",
    "X=pd.get_dummies(x.drop(columns=['building_id']))\n",
    "X=pd.get_dummies(X.drop(columns=['damage']))\n",
    "Y=x['damage'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate classifier using best params selected after grid search\n",
    "clf=XGBClassifier(n_jobs=-1,n_estimators= 600, max_depth= 10,learning_rate= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training data\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add volume percentage in test data\n",
    "test_data['volume_percentage']=test_data['area_percentage']* test_data['height_percentage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test dataset\n",
    "prediction=clf.predict((pd.get_dummies(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the prediction as per submission requirement\n",
    "result=pd.DataFrame(prediction)\n",
    "result['building_id']=X_test['building_id']\n",
    "result.rename(columns={0:'damage_grade'},inplace=True)\n",
    "result=result[['building_id','damage_grade']]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 122305\n",
      "[LightGBM] [Info] Number of data points in the train set: 208480, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score -2.333567\n",
      "[LightGBM] [Info] Start training from score -0.564567\n",
      "[LightGBM] [Info] Start training from score -1.095284\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\tvalid_0's multi_error: 0.246925\n",
      "[2000]\tvalid_0's multi_error: 0.245736\n",
      "[3000]\tvalid_0's multi_error: 0.247002\n",
      "[4000]\tvalid_0's multi_error: 0.247904\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's multi_error: 0.245275\n",
      "F1-MICRO SCORE:  0.7547245831814432\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 122213\n",
      "[LightGBM] [Info] Number of data points in the train set: 208481, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -2.343166\n",
      "[LightGBM] [Info] Start training from score -0.562987\n",
      "[LightGBM] [Info] Start training from score -1.095203\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\tvalid_0's multi_error: 0.251976\n",
      "[2000]\tvalid_0's multi_error: 0.25048\n",
      "[3000]\tvalid_0's multi_error: 0.251113\n",
      "[4000]\tvalid_0's multi_error: 0.252609\n",
      "Early stopping, best iteration is:\n",
      "[1978]\tvalid_0's multi_error: 0.250173\n",
      "F1-MICRO SCORE:  0.7498273215656178\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 122411\n",
      "[LightGBM] [Info] Number of data points in the train set: 208481, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -2.341369\n",
      "[LightGBM] [Info] Start training from score -0.564496\n",
      "[LightGBM] [Info] Start training from score -1.093154\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\tvalid_0's multi_error: 0.24952\n",
      "[2000]\tvalid_0's multi_error: 0.25\n",
      "[3000]\tvalid_0's multi_error: 0.251765\n",
      "[4000]\tvalid_0's multi_error: 0.253972\n",
      "Early stopping, best iteration is:\n",
      "[1032]\tvalid_0's multi_error: 0.249156\n",
      "F1-MICRO SCORE:  0.7508442056792018\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 122348\n",
      "[LightGBM] [Info] Number of data points in the train set: 208481, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -2.335205\n",
      "[LightGBM] [Info] Start training from score -0.564269\n",
      "[LightGBM] [Info] Start training from score -1.095318\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\tvalid_0's multi_error: 0.249751\n",
      "[2000]\tvalid_0's multi_error: 0.250173\n",
      "[3000]\tvalid_0's multi_error: 0.249002\n",
      "[4000]\tvalid_0's multi_error: 0.250959\n",
      "[5000]\tvalid_0's multi_error: 0.25165\n",
      "Early stopping, best iteration is:\n",
      "[2914]\tvalid_0's multi_error: 0.248446\n",
      "F1-MICRO SCORE:  0.7515541059094397\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 122325\n",
      "[LightGBM] [Info] Number of data points in the train set: 208481, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score -2.342567\n",
      "[LightGBM] [Info] Start training from score -0.563830\n",
      "[LightGBM] [Info] Start training from score -1.093942\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\tvalid_0's multi_error: 0.246873\n",
      "[2000]\tvalid_0's multi_error: 0.245395\n",
      "[3000]\tvalid_0's multi_error: 0.248312\n",
      "[4000]\tvalid_0's multi_error: 0.250096\n",
      "Early stopping, best iteration is:\n",
      "[1555]\tvalid_0's multi_error: 0.244935\n",
      "F1-MICRO SCORE:  0.7550652340752111\n"
     ]
    }
   ],
   "source": [
    "y = np.array(train_y[\"damage_grade\"])-1\n",
    "\n",
    "df = train_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    lgb_params = {\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"num_class\":3,\n",
    "        \"metric\" : \"multi_error\",\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"max_depth\" : -1,\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"min_sum_hessian_in_leaf\" : 0.1,\n",
    "        \"max_bin\":8192,\n",
    "        \"verbosity\" : 1,\n",
    "        \"num_threads\":6,\n",
    "        \"seed\": SEED\n",
    "    }\n",
    "\n",
    "    x_train, x_val, y_train, y_val= x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    val_data   = lgb.Dataset(x_val, label=y_val)\n",
    "\n",
    "    lgb_clf = lgb.train(lgb_params,\n",
    "                        train_data,\n",
    "                        20000,\n",
    "                        valid_sets = [val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000)\n",
    "\n",
    "    y_pred = lgb_clf.predict(x_val)\n",
    "    print(\"F1-MICRO SCORE: \", f1_score(np.array(pd.get_dummies(y_val)), threshold_arr(y_pred), average='micro'))\n",
    "    lgb_clf.save_model(f'models/model{ix}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-MICRO SCORE:  0.8166392300873749\n",
      "F1-MICRO SCORE:  0.8194020744356315\n",
      "F1-MICRO SCORE:  0.7932548225064371\n",
      "F1-MICRO SCORE:  0.838546283398759\n",
      "F1-MICRO SCORE:  0.8092793197263249\n"
     ]
    }
   ],
   "source": [
    "# Load all LightGB Models and concatenate.\n",
    "models = []\n",
    "for i in range(5):\n",
    "    model = lgb.Booster(model_file=f'models/model{i}.txt')\n",
    "\n",
    "    y_pred = model.predict(x)\n",
    "    score  = f1_score(np.array(pd.get_dummies(y)), threshold_arr(y_pred), average='micro')\n",
    "    print(\"F1-MICRO SCORE: \", score)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, x):\n",
    "    # Ensemble K-Fold CV models with adding all confidence score by class.\n",
    "    y_preds = []\n",
    "    \n",
    "    for model in models:\n",
    "        y_pred = model.predict(x)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    init_y_pred = y_preds[0]\n",
    "    for ypred in y_preds[1:]:\n",
    "        init_y_pred += ypred\n",
    "        \n",
    "    y_pred = threshold_arr(init_y_pred)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data.drop([\"building_id\"], axis=1)\n",
    "x = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble(models, x)\n",
    "y_pred = y_pred.argmax(axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_csv[\"damage_grade\"] = y_pred\n",
    "sub_csv.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439d17698fad50c487b91f94df2b17e201cd82741339cbc1d5e3266386993168"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
